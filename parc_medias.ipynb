{"cells":[{"cell_type":"markdown","metadata":{"id":"9OpZkDcSoi7p"},"source":["Parsing of all medias in the form of {name, country, blues, factuality}\n","\n","The second script (after that) adds a link to the name field"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AuaUvp8nJkg"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","import json\n","\n","def main(i):\n","    url = f\"https://mediabiasfactcheck.com/filtered-search/?pg={i}\"  # Замените на нужную ссылку\n","\n","    response = requests.get(url)\n","    html = response.text\n","\n","    soup = BeautifulSoup(html, 'html.parser')\n","    table = soup.find('table', class_=\"mbfc-table\")\n","    rows = table.find_all('tr')\n","\n","    for row in rows:\n","        cells = row.find_all('td')\n","        cell_values = [cell.get_text(strip=True) for cell in cells]\n","        if cell_values:\n","            print(cell_values[0], cell_values[1], cell_values[2], cell_values[3])\n","            data = {\n","                \"name\": cell_values[0],\n","                \"country\": cell_values[3],\n","                \"bias\": cell_values[1],\n","                \"factuality\": cell_values[2]\n","            }\n","            cell_values[0] = (cell_values[0]).replace(\"-\", '%').replace('\\\\','!').replace(' ', '_').replace('/', '$')\n","            with open(f\"labels/{cell_values[0]}.json\", \"w\", encoding=\"utf-8\") as file:\n","                json.dump(data, file, indent=4)\n","\n","for i in range(50, 69):\n","    main(i)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oDvmsneTqI9g"},"source":["Parsing of all media links, in the form of a pair (name, link)\n","\n","The links file.json is simply copied from the archive to mediabiasfactcheck.com"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-9A3LTfqGsb"},"outputs":[],"source":["import json\n","import os\n","\n","try:\n","    with open(\"links.json\", \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","except json.decoder.JSONDecodeError as e:\n","    error_position = e.pos\n","    error_character = e.doc[e.pos]\n","    print(f\"Проблемный символ: '{error_character}' (позиция {error_position})\")\n","\n","\n","with open(\"links_.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"PyrTc8fOrA1J"},"source":["Assembling a full-fledged json file with the link field"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7Yt69Cwq_9j"},"outputs":[],"source":["import json\n","import os\n","\n","count = 0\n","# connecting to an old dataset, removing repetitions\n","dir1 = \"labels\"\n","dir2 = \"links\"\n","for file1 in os.listdir(dir1):\n","    for file2 in os.listdir(dir2):\n","\n","            if(file1 == file2):\n","                try:\n","\n","                    count += 1\n","                    with open(os.path.join(dir1, file1), \"r\", encoding=\"utf-8\") as f1:\n","                        data1 = json.load(f1)\n","                    with open(os.path.join(dir2, file2), \"r\", encoding=\"utf-8\") as f2:\n","                        data2 = json.load(f2)\n","                    data = {\n","                        \"name\": data1[\"name\"],\n","                        \"country\": data1[\"country\"],\n","                        \"bias\": data1['bias'],\n","                        \"factuality\": data1['factuality'],\n","                        \"link\": data2['link']\n","                    }\n","                    name = file1\n","                    with open(f\"no_repeat/{name}\", \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(data, f, indent=4)\n","                except json.JSONDecodeError:\n","                    print(f\"ERR WITH {file1} and {file2}\")\n","\n","\n","print(count)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4z_UOjMtAkQ"},"outputs":[],"source":["# # parsing content from the first page of the media site (10-15 articles on average)\n","import os\n","import requests\n","from bs4 import BeautifulSoup\n","import json\n","\n","dataset_dir = 'no_repeat'\n","#dataset_dir = 'test'\n","json_files = [f for f in os.listdir(dataset_dir) if f.endswith('.json')]\n","\n","def get_data(url):\n","    try:\n","        response = requests.get(url, timeout=3)\n","        text = response.text\n","        content = BeautifulSoup(text, 'html.parser').get_text()\n","        return content\n","    except Exception as e:\n","        print(f\"Bad link in {url}\")\n","        with open(\"log.txt\", 'a', encoding='utf-8') as f:\n","             f.write(f\"Bad link in {url}\")\n","small_control = []\n","for json_file in json_files:\n","    with open(os.path.join(dataset_dir, json_file), 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        url = (data['link']).replace(\"https://\", \"\")\n","        url = \"https://\" + url\n","        content = get_data(url)\n","        if content:\n","            data['content'] = content\n","\n","            # (if the text is too small, we delete it)\n","            # this means that it is not possible to automate parsing\n","            # tc or the media site is blocked\n","            # or an agreement for data processing and cookies pops up\n","            # and the text of either the error or this agreement is parsed\n","            # such garbage parsing is not needed. Delete\n","\n","            if len(content) < 150:\n","                print(f\"small content in {json_file}\")\n","                small_control.append(json_file)\n","\n","        else:\n","            data['content'] = \"\"\n","\n","    with open(os.path.join(dataset_dir, json_file), 'w', encoding='utf-8') as f:\n","        json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","    print(f\"Content added to {json_file}.\")\n","\n","print(small_control)\n","print(len(small_control))"]},{"cell_type":"markdown","metadata":{"id":"VmFRRFAjrjiw"},"source":["Пост обработка.\n","\n","Добавление numeric к bias и factuality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wf7-K-GMrmAg"},"outputs":[],"source":["import json\n","import os\n","\n","directory = 'gold'\n","def add_num_once(filep):\n","    file = os.path.join(filep)\n","   \n","    with open(file, 'r', encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","# Adding numeric fields to existing text fields\n","    try:\n","        bias_text = data['bias']\n","\n","        if bias_text == 'Left':\n","            data['bias (numeric)'] = 0\n","        elif bias_text == 'Left-Center':\n","            data['bias (numeric)'] = 1\n","        elif bias_text == 'Center' or bias_text == 'Least Biased':\n","            data['bias (numeric)'] = 2\n","        elif bias_text == 'Right-Center':\n","            data['bias (numeric)'] = 3\n","        elif bias_text == 'Right':\n","            data['bias (numeric)'] = 4\n","        else :\n","            data['bias (numeric)'] = -1\n","    except IndexError:\n","        print(file)\n","\n","    try:\n","        factuality_text = data['factuality']\n","        if factuality_text == 'Very High':\n","            data['factuality (numeric)'] = 0\n","        elif factuality_text == 'High':\n","            data['factuality (numeric)'] = 1\n","        elif factuality_text == 'Mostly Factual':\n","            data['factuality (numeric)'] = 2\n","        elif factuality_text == 'Mixed':\n","            data['factuality (numeric)'] = 3\n","        elif factuality_text == 'Low':\n","            data['factuality (numeric)'] = 4\n","        elif factuality_text == 'Very Low':\n","            data['factuality (numeric)'] = 5\n","        else:\n","            data['factuality (numeric)'] = -1\n","    except IndexError:\n","        print(file)\n","   # data[0]['Link'] = 'https://' + data[0]['Link']\n","\n","    with open(file, 'w') as f:\n","        json.dump(data, f, indent=4)\n","\n","\n","for filename in os.listdir(directory):\n","  \n","    filepath = os.path.join(directory, filename)\n","   \n","    if filepath.endswith('.json'):\n","        add_num_once(filepath)"]},{"cell_type":"markdown","metadata":{"id":"QHugmR4qr0rl"},"source":["Randomization of the name and id field"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmoX31alr3mB"},"outputs":[],"source":["import os\n","import random\n","import string\n","import json\n","\n","folder_path = 'dir'\n","\n","\n","for filename in os.listdir(folder_path):\n","    old_file_name = filename\n","\n","\n","    new_file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) + '.json'\n","    new_file_path = os.path.join(folder_path, new_file_name)\n","\n","\n","    os.rename(folder_path + '/' + old_file_name, new_file_path)\n","\n","    with open(new_file_path, 'r+', encoding='utf-8') as f:\n","        data = json.load(f)\n","        data['id'] = new_file_name[:-5]\n","        f.seek(0)\n","        json.dump(data, f, indent=2)\n","        f.truncate()"]},{"cell_type":"markdown","metadata":{"id":"_u5ZhhlVsFiK"},"source":["Bringing the file to the desired form.\n","\n","Setting up the order of fields for convenient parsing quality control"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poQ-I6WRsFIh"},"outputs":[],"source":["import os\n","import json\n","import shutil\n","import random\n","import string\n","\n","path = 'gold'\n","\n","for filename in os.listdir(path):\n","\n","    if filename.endswith(\".json\"):\n","\n","        with open(os.path.join(path, filename), \"r\", encoding='utf-8') as f:\n","            content = json.load(f)\n","        data = {\n","            \"id\": ''.join(random.choices(string.ascii_letters + string.digits, k=8)),\n","            \"name\": content[\"name\"],\n","            \"country\": content[\"country\"],\n","            \"bias\": content[\"bias\"],\n","            \"bias (numeric)\": content[\"bias (numeric)\"],\n","            \"factuality\": content[\"factuality\"],\n","            \"factuality (numeric)\": content[\"factuality (numeric)\"],\n","            \"link\": content[\"link\"],\n","            \"content\": content[\"content\"]\n","        }\n","        with open(os.path.join(path, filename), \"w\", encoding='utf-8') as f:\n","            json.dump(data, f, indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["Later, a new parsing strategy was chosen. \n","\n","It consists in collecting all links from the main media site, removing among them those that lead to another domain and those that are less than 50 characters long (these are links from the menu and junk links to advertising). It is assumed that after such a selection, links to articles remain. \n","\n","Next, click on them and parse the text from the articles page."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOWCY1uuQpAh4z2CMlJPr8x","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
