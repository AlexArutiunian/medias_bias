{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWCY1uuQpAh4z2CMlJPr8x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Парсинг всех медий в виде {name, country, bias, factuality}\n","\n","Второй скрипт (после этого) добавляет link по полю name"],"metadata":{"id":"9OpZkDcSoi7p"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AuaUvp8nJkg"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","import json\n","\n","def main(i):\n","    url = f\"https://mediabiasfactcheck.com/filtered-search/?pg={i}\"  # Замените на нужную ссылку\n","\n","    response = requests.get(url)\n","    html = response.text\n","\n","    soup = BeautifulSoup(html, 'html.parser')\n","    table = soup.find('table', class_=\"mbfc-table\")\n","    rows = table.find_all('tr')\n","\n","    for row in rows:\n","        cells = row.find_all('td')\n","        cell_values = [cell.get_text(strip=True) for cell in cells]\n","        if cell_values:\n","            print(cell_values[0], cell_values[1], cell_values[2], cell_values[3])\n","            data = {\n","                \"name\": cell_values[0],\n","                \"country\": cell_values[3],\n","                \"bias\": cell_values[1],\n","                \"factuality\": cell_values[2]\n","            }\n","            cell_values[0] = (cell_values[0]).replace(\"-\", '%').replace('\\\\','!').replace(' ', '_').replace('/', '$')\n","            with open(f\"labels/{cell_values[0]}.json\", \"w\", encoding=\"utf-8\") as file:\n","                json.dump(data, file, indent=4)\n","\n","for i in range(50, 69):\n","    main(i)\n","\n"]},{"cell_type":"markdown","source":["Парсинг всех ссылок на медии, в виде пары (name, link)\n","\n","Файл links.json просто скопирован из архива на mediabiasfactcheck.com"],"metadata":{"id":"oDvmsneTqI9g"}},{"cell_type":"code","source":["import json\n","import os\n","\n","try:\n","    with open(\"links.json\", \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","except json.decoder.JSONDecodeError as e:\n","    error_position = e.pos\n","    error_character = e.doc[e.pos]\n","    print(f\"Проблемный символ: '{error_character}' (позиция {error_position})\")\n","\n","\n","with open(\"links_.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, indent=4)"],"metadata":{"id":"S-9A3LTfqGsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Сборка полноценного json файла с полем link"],"metadata":{"id":"PyrTc8fOrA1J"}},{"cell_type":"code","source":["import json\n","import os\n","\n","count = 0\n","\n","# соединение со старым датасетом, убирая повторы\n","dir1 = \"labels\"\n","dir2 = \"links\"\n","for file1 in os.listdir(dir1):\n","    for file2 in os.listdir(dir2):\n","\n","            if(file1 == file2):\n","                try:\n","\n","                    count += 1\n","                    with open(os.path.join(dir1, file1), \"r\", encoding=\"utf-8\") as f1:\n","                        data1 = json.load(f1)\n","                    with open(os.path.join(dir2, file2), \"r\", encoding=\"utf-8\") as f2:\n","                        data2 = json.load(f2)\n","                    data = {\n","                        \"name\": data1[\"name\"],\n","                        \"country\": data1[\"country\"],\n","                        \"bias\": data1['bias'],\n","                        \"factuality\": data1['factuality'],\n","                        \"link\": data2['link']\n","                    }\n","                    name = file1\n","                    with open(f\"no_repeat/{name}\", \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(data, f, indent=4)\n","                except json.JSONDecodeError:\n","                    print(f\"ERR WITH {file1} and {file2}\")\n","\n","\n","print(count)\n"],"metadata":{"id":"p7Yt69Cwq_9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# парсинг контента с первой страницы сайта медии (в среднем 10-15 статей)\n","\n","import os\n","import requests\n","from bs4 import BeautifulSoup\n","import json\n","\n","dataset_dir = 'no_repeat'\n","#dataset_dir = 'test'\n","json_files = [f for f in os.listdir(dataset_dir) if f.endswith('.json')]\n","\n","def get_data(url):\n","    try:\n","        response = requests.get(url, timeout=3)\n","        text = response.text\n","        content = BeautifulSoup(text, 'html.parser').get_text()\n","        return content\n","    except Exception as e:\n","        print(f\"Bad link in {url}\")\n","        with open(\"log.txt\", 'a', encoding='utf-8') as f:\n","             f.write(f\"Bad link in {url}\")\n","small_control = []\n","for json_file in json_files:\n","    with open(os.path.join(dataset_dir, json_file), 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        url = (data['link']).replace(\"https://\", \"\")\n","        url = \"https://\" + url\n","        content = get_data(url)\n","        if content:\n","            data['content'] = content\n","\n","            #(если текст слишком маленький - удаляем)\n","            # это значит что автоматизировать парсинг не получается\n","            # тк либо сайт медии заблокирован\n","            # либо выскакивает соглашение на обработку данных и cookie\n","            # и парсится текст либо ошибки либо этого соглашения\n","            # такой мусорный парсинг не нужен. Удаляем\n","\n","            if len(content) < 150:\n","                print(f\"small content in {json_file}\")\n","                small_control.append(json_file)\n","\n","        else:\n","            data['content'] = \"\"\n","\n","    with open(os.path.join(dataset_dir, json_file), 'w', encoding='utf-8') as f:\n","        json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","    print(f\"Content added to {json_file}.\")\n","\n","print(small_control)\n","print(len(small_control))"],"metadata":{"id":"L4z_UOjMtAkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Пост обработка.\n","\n","Добавление numeric к bias и factuality"],"metadata":{"id":"VmFRRFAjrjiw"}},{"cell_type":"code","source":["import json\n","import os\n","\n","directory = 'gold'\n","def add_num_once(filep):\n","    file = os.path.join(filep)\n","    # Открываем файл и загружаем его содержимое в словарь\n","    with open(file, 'r', encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    # Добавляем числовые поля к существующим текстовым полям\n","    try:\n","        bias_text = data['bias']\n","\n","        if bias_text == 'Left':\n","            data['bias (numeric)'] = 0\n","        elif bias_text == 'Left-Center':\n","            data['bias (numeric)'] = 1\n","        elif bias_text == 'Center' or bias_text == 'Least Biased':\n","            data['bias (numeric)'] = 2\n","        elif bias_text == 'Right-Center':\n","            data['bias (numeric)'] = 3\n","        elif bias_text == 'Right':\n","            data['bias (numeric)'] = 4\n","        else :\n","            data['bias (numeric)'] = -1\n","    except IndexError:\n","        print(file)\n","\n","    try:\n","        factuality_text = data['factuality']\n","        if factuality_text == 'Very High':\n","            data['factuality (numeric)'] = 0\n","        elif factuality_text == 'High':\n","            data['factuality (numeric)'] = 1\n","        elif factuality_text == 'Mostly Factual':\n","            data['factuality (numeric)'] = 2\n","        elif factuality_text == 'Mixed':\n","            data['factuality (numeric)'] = 3\n","        elif factuality_text == 'Low':\n","            data['factuality (numeric)'] = 4\n","        elif factuality_text == 'Very Low':\n","            data['factuality (numeric)'] = 5\n","        else:\n","            data['factuality (numeric)'] = -1\n","    except IndexError:\n","        print(file)\n","   # data[0]['Link'] = 'https://' + data[0]['Link']\n","    # Сохраняем измененный словарь обратно в JSON файл\n","    with open(file, 'w') as f:\n","        json.dump(data, f, indent=4)\n","\n","\n","for filename in os.listdir(directory):\n","    # Объединяем имя файла и путь к директории\n","    filepath = os.path.join(directory, filename)\n","    # Проверяем, является ли файл JSON-файлом\n","    if filepath.endswith('.json'):\n","        add_num_once(filepath)"],"metadata":{"id":"Wf7-K-GMrmAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Рандомизация имени и поля id"],"metadata":{"id":"QHugmR4qr0rl"}},{"cell_type":"code","source":["import os\n","import random\n","import string\n","import json\n","\n","# Указываем путь к папке, где находятся файлы\n","folder_path = 'dir'\n","\n","# Проходимся по всем файлам в папке\n","for filename in os.listdir(folder_path):\n","    old_file_name = filename\n","\n","    # Генерируем случайный id\n","    new_file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) + '.json'\n","    new_file_path = os.path.join(folder_path, new_file_name)\n","\n","    # Переименовываем файл\n","    os.rename(folder_path + '/' + old_file_name, new_file_path)\n","\n","    # Обновляем id в файле\n","    with open(new_file_path, 'r+', encoding='utf-8') as f:\n","        data = json.load(f)\n","        data['id'] = new_file_name[:-5]\n","        f.seek(0)\n","        json.dump(data, f, indent=2)\n","        f.truncate()"],"metadata":{"id":"VmoX31alr3mB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Приведение файла к нужному виду.\n","\n","Настройка порядка полей для удобной проверки качества парсинга"],"metadata":{"id":"_u5ZhhlVsFiK"}},{"cell_type":"code","source":["import os\n","import json\n","import shutil\n","import random\n","import string\n","\n","\n","# Путь к директории с JSON-файлами\n","path = 'gold'\n","\n","# Проходимся по всем файлам в директории\n","for filename in os.listdir(path):\n","    # Проверяем, что файл имеет расширение .json\n","    if filename.endswith(\".json\"):\n","        # Считываем содержимое файла\n","        with open(os.path.join(path, filename), \"r\", encoding='utf-8') as f:\n","            content = json.load(f)\n","        data = {\n","            \"id\": ''.join(random.choices(string.ascii_letters + string.digits, k=8)),\n","            \"name\": content[\"name\"],\n","            \"country\": content[\"country\"],\n","            \"bias\": content[\"bias\"],\n","            \"bias (numeric)\": content[\"bias (numeric)\"],\n","            \"factuality\": content[\"factuality\"],\n","            \"factuality (numeric)\": content[\"factuality (numeric)\"],\n","            \"link\": content[\"link\"],\n","            \"content\": content[\"content\"]\n","        }\n","        with open(os.path.join(path, filename), \"w\", encoding='utf-8') as f:\n","            json.dump(data, f, indent=4)"],"metadata":{"id":"poQ-I6WRsFIh"},"execution_count":null,"outputs":[]}]}